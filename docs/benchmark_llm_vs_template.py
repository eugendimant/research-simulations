#!/usr/bin/env python3
"""
LLM vs Template Engine: Side-by-Side Benchmark Runner
======================================================

Run this script locally or on any machine with internet access to generate
a full comparison of all 3 LLM providers vs the template engine.

Usage:
    cd research-simulations
    python3 docs/benchmark_llm_vs_template.py

Output:
    - Prints side-by-side comparison to stdout
    - Writes docs/llm_vs_template_benchmark.md with full results

Requirements:
    - Python 3.8+
    - Internet access (for LLM API calls)
    - No additional packages needed (uses stdlib urllib)
"""

import sys
import os
import time
import random
import json

# Ensure simulation_app is importable
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'simulation_app'))
os.chdir(os.path.join(os.path.dirname(__file__), '..'))

from utils.response_library import ComprehensiveResponseGenerator
from utils.llm_response_generator import (
    _call_llm_api, _parse_json_responses, _build_batch_prompt, SYSTEM_PROMPT,
    GROQ_API_URL, GROQ_MODEL, _DEFAULT_GROQ_KEY,
    CEREBRAS_API_URL, CEREBRAS_MODEL, _DEFAULT_CEREBRAS_KEY,
    OPENROUTER_API_URL, OPENROUTER_MODEL, _DEFAULT_OPENROUTER_KEY,
)

# ── Questions from real QSF files ──────────────────────────────────────
QUESTIONS = [
    {
        "id": "Q1", "type": "Description",
        "source": "Coffee_Shop_Loyalty_Programs.qsf",
        "study": "Coffee Shop Loyalty Programs",
        "condition": "Points-based loyalty program (200 points for reward)",
        "text": "In 1-2 sentences, please describe what you see in the loyalty program above:",
    },
    {
        "id": "Q2", "type": "Evaluation",
        "source": "0401_Backlash_aftermath_performance_pilot.qsf",
        "study": "Gender Backlash in Salary Negotiation",
        "condition": "Female candidate / assertive negotiation style",
        "text": "What is your overall impression of the job candidate based on how they negotiated the offer?",
    },
    {
        "id": "Q3", "type": "Explanation",
        "source": "DG & PGG (Information Nudge).qsf",
        "study": "Dictator Game with Information Nudge",
        "condition": "Social information nudge (told average donation is 40%)",
        "text": "Please explain your choice in a few sentences:",
    },
    {
        "id": "Q4", "type": "Opinion/Belief",
        "source": "!!Human-AI_Interaction_Template.qsf",
        "study": "Human-AI Interaction and Trust",
        "condition": "AI-generated advice condition",
        "text": "Reflecting on this, are there any specific concerns or hopes you have about AI in everyday decision-making? Please describe one below and share your reasons.",
    },
    {
        "id": "Q5", "type": "Meta/Suspicion",
        "source": "0401_Backlash_aftermath_performance_pilot.qsf",
        "study": "Gender Backlash in Salary Negotiation",
        "condition": "Male candidate / collaborative negotiation style",
        "text": "What do you think this study was about?",
    },
    {
        "id": "Q6", "type": "Feedback",
        "source": "DG & PGG (Information Nudge).qsf",
        "study": "Dictator Game with Information Nudge",
        "condition": "Control (no nudge)",
        "text": "Feedback: Was there anything confusing about the experiment? Was anything unclear? If so, please let us know.",
    },
]

# ── 5 persona profiles ────────────────────────────────────────────────
PERSONAS = [
    {"label": "Engaged (formal)",     "v": 0.75, "f": 0.80, "e": 0.90, "sentiment": "positive"},
    {"label": "Casual (brief)",       "v": 0.25, "f": 0.15, "e": 0.55, "sentiment": "neutral"},
    {"label": "Thoughtful (verbose)", "v": 0.85, "f": 0.60, "e": 0.85, "sentiment": "negative"},
    {"label": "Satisficer (minimal)", "v": 0.10, "f": 0.40, "e": 0.20, "sentiment": "neutral"},
    {"label": "Enthusiastic (casual)","v": 0.65, "f": 0.20, "e": 0.90, "sentiment": "very_positive"},
]

PROVIDERS = [
    ("Groq", GROQ_API_URL, GROQ_MODEL, _DEFAULT_GROQ_KEY),
    ("Cerebras", CEREBRAS_API_URL, CEREBRAS_MODEL, _DEFAULT_CEREBRAS_KEY),
    ("OpenRouter", OPENROUTER_API_URL, OPENROUTER_MODEL, _DEFAULT_OPENROUTER_KEY),
]


def main() -> None:
    template_gen = ComprehensiveResponseGenerator()
    persona_specs = [
        {"verbosity": p["v"], "formality": p["f"], "engagement": p["e"], "sentiment": p["sentiment"]}
        for p in PERSONAS
    ]

    lines = []
    lines.append("# LLM vs Template Engine: Side-by-Side Benchmark")
    lines.append("")
    lines.append(f"**Generated**: {time.strftime('%Y-%m-%d %H:%M UTC')}")
    lines.append("**Version**: 1.4.10")
    lines.append("")
    lines.append("## Overview")
    lines.append("")
    lines.append("This document compares open-ended survey responses generated by **three LLM providers**")
    lines.append("and the **built-in template engine** (225+ domains, 40 question types) across 6 different")
    lines.append("question types extracted from real Qualtrics QSF files used in behavioral science courses.")
    lines.append("")
    lines.append("### Providers Tested")
    lines.append("")
    lines.append("| Provider | Model | Free Tier |")
    lines.append("|----------|-------|-----------|")
    lines.append(f"| **Groq** (built-in) | `{GROQ_MODEL}` | 14,400 requests/day |")
    lines.append(f"| **Cerebras** (built-in) | `{CEREBRAS_MODEL}` | 1M tokens/day |")
    lines.append(f"| **OpenRouter** (built-in) | `{OPENROUTER_MODEL}` | Free models |")
    lines.append("| **Template Engine** (fallback) | N/A — rule-based | Unlimited, no API needed |")
    lines.append("")
    lines.append("### Persona Profiles Used")
    lines.append("")
    lines.append("| # | Persona | Verbosity | Formality | Engagement | Sentiment |")
    lines.append("|---|---------|-----------|-----------|------------|-----------|")
    for i, p in enumerate(PERSONAS, 1):
        lines.append(f"| {i} | {p['label']} | {p['v']:.2f} | {p['f']:.2f} | {p['e']:.2f} | {p['sentiment']} |")
    lines.append("")
    lines.append("---")
    lines.append("")

    overall_stats = {}

    for q in QUESTIONS:
        print(f"\n{'='*70}")
        print(f"{q['id']}: {q['type']} — {q['text'][:60]}...")
        print(f"{'='*70}")

        lines.append(f"## {q['id']}: {q['type']} Question")
        lines.append("")
        lines.append(f"**Source QSF**: `{q['source']}`  ")
        lines.append(f"**Study**: {q['study']}  ")
        lines.append(f"**Condition**: {q['condition']}  ")
        lines.append(f"**Question**: *\"{q['text']}\"*")
        lines.append("")

        # ── Template engine ────────────────────────────────────────
        t0 = time.time()
        template_resps = []
        for i, p in enumerate(PERSONAS):
            resp = template_gen.generate(
                question_text=q["text"],
                sentiment=p["sentiment"],
                persona_verbosity=p["v"],
                persona_formality=p["f"],
                persona_engagement=p["e"],
                condition=q["condition"],
                question_name=q["id"],
                participant_seed=42 + i,
            )
            template_resps.append(resp)
        template_time = time.time() - t0

        lines.append("### Template Engine")
        lines.append("")
        print(f"\n  Template Engine ({template_time:.3f}s):")
        for i, (resp, persona) in enumerate(zip(template_resps, PERSONAS)):
            lines.append(f"**Persona {i+1}** ({persona['label']}, {persona['sentiment']}):")
            lines.append(f"> {resp}")
            lines.append("")
            print(f"    [{i+1}] {resp[:120]}")

        template_avg = sum(len(r) for r in template_resps) / max(1, len(template_resps))
        template_unique = len(set(template_resps)) / max(1, len(template_resps)) * 100
        lines.append(f"*Stats: {template_time:.3f}s, avg {template_avg:.0f} chars, {template_unique:.0f}% unique*")
        lines.append("")

        # ── LLM providers ──────────────────────────────────────────
        for pname, url, model, key in PROVIDERS:
            prompt = _build_batch_prompt(
                question_text=q["text"],
                condition=q["condition"],
                study_title=q["study"],
                study_description=f"A behavioral experiment examining {q['study'].lower()}.",
                persona_specs=persona_specs,
            )

            t0 = time.time()
            raw = _call_llm_api(url, key, model, SYSTEM_PROMPT, prompt,
                                temperature=0.7, max_tokens=4000, timeout=45)
            elapsed = time.time() - t0

            if raw:
                parsed = _parse_json_responses(raw, 5)
                lines.append(f"### {pname} (`{model}`)")
                lines.append("")
                print(f"\n  {pname} ({elapsed:.1f}s, {len(parsed)} responses):")
                for i, resp in enumerate(parsed):
                    p = PERSONAS[i] if i < len(PERSONAS) else PERSONAS[-1]
                    lines.append(f"**Persona {i+1}** ({p['label']}, {p['sentiment']}):")
                    lines.append(f"> {resp}")
                    lines.append("")
                    print(f"    [{i+1}] {resp[:120]}")

                avg = sum(len(r) for r in parsed) / max(1, len(parsed))
                unique = len(set(parsed)) / max(1, len(parsed)) * 100
                lines.append(f"*Stats: {elapsed:.1f}s, avg {avg:.0f} chars, {unique:.0f}% unique*")
                lines.append("")

                key_name = f"{pname}_{q['id']}"
                overall_stats.setdefault(pname, []).append({"time": elapsed, "avg_len": avg, "unique": unique, "count": len(parsed)})
            else:
                lines.append(f"### {pname} (`{model}`)")
                lines.append("")
                lines.append(f"*API call failed ({elapsed:.1f}s) — provider may be rate-limited or unreachable from this environment.*")
                lines.append("")
                print(f"\n  {pname}: FAILED ({elapsed:.1f}s)")

        overall_stats.setdefault("Template", []).append({
            "time": template_time, "avg_len": template_avg, "unique": template_unique, "count": len(template_resps),
        })

        lines.append("---")
        lines.append("")

    # ── Summary table ──────────────────────────────────────────────
    lines.append("## Aggregate Statistics")
    lines.append("")
    lines.append("| Provider | Avg Time/Question | Avg Response Length | Avg Uniqueness | Questions Answered |")
    lines.append("|----------|-------------------|--------------------|----------------|-------------------|")

    for provider_name in ["Groq", "Cerebras", "OpenRouter", "Template"]:
        stats = overall_stats.get(provider_name, [])
        if stats:
            avg_time = sum(s["time"] for s in stats) / len(stats)
            avg_len = sum(s["avg_len"] for s in stats) / len(stats)
            avg_uniq = sum(s["unique"] for s in stats) / len(stats)
            answered = sum(1 for s in stats if s["count"] > 0)
            lines.append(f"| {provider_name} | {avg_time:.2f}s | {avg_len:.0f} chars | {avg_uniq:.0f}% | {answered}/{len(QUESTIONS)} |")
        else:
            lines.append(f"| {provider_name} | N/A | N/A | N/A | 0/{len(QUESTIONS)} |")
    lines.append("")

    lines.append("## Architecture Comparison")
    lines.append("")
    lines.append("| Dimension | LLM Providers | Template Engine |")
    lines.append("|-----------|---------------|-----------------|")
    lines.append("| **How it works** | Sends persona-parameterized prompts to Llama 3.3 70B | Selects from 225+ domain templates, applies 7-layer persona variation |")
    lines.append("| **Response quality** | High — natural, context-aware, condition-specific | Good — domain-matched, persona-varied, grammatically correct |")
    lines.append("| **Persona fidelity** | Excellent — LLM follows verbosity/formality/engagement | Good — template selection + post-processing transformations |")
    lines.append("| **Condition reference** | Often references experimental condition explicitly | Uses condition for template category selection |")
    lines.append("| **Speed** | 2-5s per batch of 20 | <0.001s per response |")
    lines.append("| **Reliability** | 3-provider chain (Groq → Cerebras → OpenRouter) | 100% — no external dependencies |")
    lines.append("| **Cost** | Free (built-in keys, ~14K+ requests/day combined) | Free |")
    lines.append("")

    doc = "\n".join(lines)
    outpath = os.path.join(os.path.dirname(__file__), "llm_vs_template_benchmark.md")
    with open(outpath, "w") as f:
        f.write(doc)
    print(f"\n\nBenchmark document written to: {outpath}")
    print(f"Total lines: {len(lines)}")


if __name__ == "__main__":
    main()
